{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b47a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from pydataset import data\n",
    "import numpy as np\n",
    "import env\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import prepare\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd657c6e",
   "metadata": {},
   "source": [
    "Fit a K-Nearest Neighbors classifier to your training sample and transform (i.e. make predictions on the training sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a46c6",
   "metadata": {},
   "source": [
    "Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8349b1",
   "metadata": {},
   "source": [
    "Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e20dda",
   "metadata": {},
   "source": [
    "Run through steps 1-3 setting k to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae9559",
   "metadata": {},
   "source": [
    "Run through steps 1-3 setting k to 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74a25e5",
   "metadata": {},
   "source": [
    "What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f027a5",
   "metadata": {},
   "source": [
    "Which model performs best on our out-of-sample data from validate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
